{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests, json, sys, math, time, pickle, newspaper, cnfg\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from newspaper import Article\n",
    "from nytimesarticle import articleAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "#logging.basicConfig(filename='fox_error_log.log',filemode='w', level=logging.INFO)\n",
    "logging.basicConfig(filename='nyt_error_log.log',filemode='w', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nyt_api_config = cnfg.load(\".nyt_api_config\")\n",
    "nyt_api_key = nyt_api_config['api_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to get URLs of refugee articles, and then pull those articles from APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_fox_urls():\n",
    "    df = pd.DataFrame()\n",
    "    for start in range(0, 2074, 10):  # 2074 articles match my criteria. API returns 10 at a time.\n",
    "        url = 'http://api.foxnews.com/v1/content/search?q=refugees&min_date=2015-01-01&max_date=2015-12-31&type=article&start=' \\\n",
    "                    + str(start)\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            msg = 'Get response from start position %d failed: %d' % (start, response.status_code)\n",
    "            logging.error(msg, exc_info=True)\n",
    "        else:\n",
    "            res = json.loads(response.text)\n",
    "            df = df.append(pd.DataFrame.from_dict(res['response']['docs']), ignore_index=True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def get_fox_text(urls, t):\n",
    "    config=newspaper.Config()\n",
    "    config.fetch_images = False\n",
    "    \n",
    "    for i in range(len(urls)):\n",
    "        msg = 'Downloading url %d' % i\n",
    "        logging.info(msg)\n",
    "        a = Article(urls[i], config=config) \n",
    "        try:\n",
    "            a.download()\n",
    "            a.parse()\n",
    "            t.append(a.text)\n",
    "        except:\n",
    "            t.append('')\n",
    "            msg = 'Get text from %s failed:' % url[i]\n",
    "            logging.error(msg, exc_info=True)\n",
    "            \n",
    "            \n",
    "def get_nyt_urls():\n",
    "    df = pd.DataFrame()\n",
    "    api = articleAPI(nyt_api_key)\n",
    "    for i in range(122): # 1579 articles match my criteria. The API only allows getting 1200.\n",
    "        try:\n",
    "            response = api.search(q = 'refugees', \n",
    "            fq = {'NOT type_of_material':'video',\n",
    "                 'source':'The New York Times'}, \n",
    "                begin_date = 20150101,\n",
    "                end_date = 20151231,\n",
    "                fl = 'web_url,abstract,source,headline,pub_date,document_type,news_desk,type_of_material',\n",
    "                page = i)\n",
    "        except:\n",
    "            msg = 'Get response from offset %d failed. Response status: %s' % (i, response['status'])\n",
    "            logging.error(msg, exc_info=True)\n",
    "        try:\n",
    "            df = df.append(pd.DataFrame.from_dict(response['response']['docs']), ignore_index=True)\n",
    "        except:\n",
    "            msg = 'Appending response from offset %d failed. Response status: %s' % (i, response['status'])\n",
    "            logging.error(msg, exc_info=True)\n",
    "        time.sleep(1)\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_nyt_text(urls, t):\n",
    "    config=newspaper.Config()\n",
    "    config.fetch_images = False\n",
    "    \n",
    "    for i in range(len(urls)):\n",
    "        msg = 'Downloading url %d' % i\n",
    "        logging.info(msg)\n",
    "        a = Article(urls[i], config=config) \n",
    "        try:\n",
    "            a.download()\n",
    "            a.parse()\n",
    "            t.append(a.text)\n",
    "        except:\n",
    "            t.append('')\n",
    "            msg = 'Get text from %s failed:' % url[i]\n",
    "            logging.error(msg, exc_info=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Fox News articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fox_urls = get_fox_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fox_urls.url = fox_urls.url.apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-11-25T13:58:12Z</td>\n",
       "      <td>&lt;p&gt;Bernie Goldberg analyzes how the media is c...</td>\n",
       "      <td>Sympathy for Syrian refugees</td>\n",
       "      <td>http://www.foxnews.com/transcript/2015/11/25/s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-11-19T14:27:12Z</td>\n",
       "      <td>&lt;p&gt;Kerry Kennedy on how the left is handling f...</td>\n",
       "      <td>Democrats, ISIS and refugees</td>\n",
       "      <td>http://www.foxnews.com/transcript/2015/11/19/d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-03T18:47:56Z</td>\n",
       "      <td>!--StartFragment--&gt;Sen.</td>\n",
       "      <td>Republicans: 12 terrorism-implicated refugees ...</td>\n",
       "      <td>http://www.foxnews.com/politics/2015/12/03/rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-11-29T11:37:14Z</td>\n",
       "      <td>Republican presidential candidate Ben Carson i...</td>\n",
       "      <td>Carson's Halloween candy solution for Syrian r...</td>\n",
       "      <td>http://www.foxnews.com/politics/2015/11/29/car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-11-29T11:16:12Z</td>\n",
       "      <td>!--StartFragment--&gt;Asked by Chuck Todd on \"Mee...</td>\n",
       "      <td>Trump: Obama plans 250k refugees in America</td>\n",
       "      <td>http://www.foxnews.com/politics/2015/11/29/tru...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   date                                        description  \\\n",
       "0  2015-11-25T13:58:12Z  <p>Bernie Goldberg analyzes how the media is c...   \n",
       "1  2015-11-19T14:27:12Z  <p>Kerry Kennedy on how the left is handling f...   \n",
       "2  2015-12-03T18:47:56Z                            !--StartFragment-->Sen.   \n",
       "3  2015-11-29T11:37:14Z  Republican presidential candidate Ben Carson i...   \n",
       "4  2015-11-29T11:16:12Z  !--StartFragment-->Asked by Chuck Todd on \"Mee...   \n",
       "\n",
       "                                               title  \\\n",
       "0                       Sympathy for Syrian refugees   \n",
       "1                       Democrats, ISIS and refugees   \n",
       "2  Republicans: 12 terrorism-implicated refugees ...   \n",
       "3  Carson's Halloween candy solution for Syrian r...   \n",
       "4        Trump: Obama plans 250k refugees in America   \n",
       "\n",
       "                                                 url  \n",
       "0  http://www.foxnews.com/transcript/2015/11/25/s...  \n",
       "1  http://www.foxnews.com/transcript/2015/11/19/d...  \n",
       "2  http://www.foxnews.com/politics/2015/12/03/rep...  \n",
       "3  http://www.foxnews.com/politics/2015/11/29/car...  \n",
       "4  http://www.foxnews.com/politics/2015/11/29/tru...  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fox_urls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('fox_urls.pkl', 'wb') as picklefile:\n",
    "#     pickle.dump(fox_urls, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('fox_urls.pkl', 'rb') as picklefile:\n",
    "#     fox_urls = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"//anaconda/lib/python2.7/site-packages/newspaper/parsers.py\", line 54, in fromstring\n",
      "    cls.doc = lxml.html.fromstring(html)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 706, in fromstring\n",
      "    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 600, in document_fromstring\n",
      "    value = etree.fromstring(html, parser, **kw)\n",
      "  File \"lxml.etree.pyx\", line 3032, in lxml.etree.fromstring (src/lxml/lxml.etree.c:68121)\n",
      "  File \"parser.pxi\", line 1786, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:102470)\n",
      "  File \"parser.pxi\", line 1667, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:101229)\n",
      "  File \"parser.pxi\", line 1035, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:96139)\n",
      "  File \"parser.pxi\", line 582, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:91290)\n",
      "  File \"parser.pxi\", line 683, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:92476)\n",
      "  File \"parser.pxi\", line 631, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:91904)\n",
      "XMLSyntaxError: line 479: htmlParseEntityRef: expecting ';'\n",
      "Traceback (most recent call last):\n",
      "  File \"//anaconda/lib/python2.7/site-packages/newspaper/parsers.py\", line 54, in fromstring\n",
      "    cls.doc = lxml.html.fromstring(html)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 706, in fromstring\n",
      "    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 600, in document_fromstring\n",
      "    value = etree.fromstring(html, parser, **kw)\n",
      "  File \"lxml.etree.pyx\", line 3032, in lxml.etree.fromstring (src/lxml/lxml.etree.c:68121)\n",
      "  File \"parser.pxi\", line 1786, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:102470)\n",
      "  File \"parser.pxi\", line 1667, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:101229)\n",
      "  File \"parser.pxi\", line 1035, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:96139)\n",
      "  File \"parser.pxi\", line 582, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:91290)\n",
      "  File \"parser.pxi\", line 683, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:92476)\n",
      "  File \"parser.pxi\", line 631, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:91904)\n",
      "XMLSyntaxError: line 479: htmlParseEntityRef: expecting ';'\n",
      "Traceback (most recent call last):\n",
      "  File \"//anaconda/lib/python2.7/site-packages/newspaper/parsers.py\", line 54, in fromstring\n",
      "    cls.doc = lxml.html.fromstring(html)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 706, in fromstring\n",
      "    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 600, in document_fromstring\n",
      "    value = etree.fromstring(html, parser, **kw)\n",
      "  File \"lxml.etree.pyx\", line 3032, in lxml.etree.fromstring (src/lxml/lxml.etree.c:68121)\n",
      "  File \"parser.pxi\", line 1786, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:102470)\n",
      "  File \"parser.pxi\", line 1667, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:101229)\n",
      "  File \"parser.pxi\", line 1035, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:96139)\n",
      "  File \"parser.pxi\", line 582, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:91290)\n",
      "  File \"parser.pxi\", line 683, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:92476)\n",
      "  File \"parser.pxi\", line 631, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:91904)\n",
      "XMLSyntaxError: line 1097: htmlParseEntityRef: expecting ';'\n",
      "Traceback (most recent call last):\n",
      "  File \"//anaconda/lib/python2.7/site-packages/newspaper/parsers.py\", line 54, in fromstring\n",
      "    cls.doc = lxml.html.fromstring(html)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 706, in fromstring\n",
      "    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 600, in document_fromstring\n",
      "    value = etree.fromstring(html, parser, **kw)\n",
      "  File \"lxml.etree.pyx\", line 3032, in lxml.etree.fromstring (src/lxml/lxml.etree.c:68121)\n",
      "  File \"parser.pxi\", line 1786, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:102470)\n",
      "  File \"parser.pxi\", line 1667, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:101229)\n",
      "  File \"parser.pxi\", line 1035, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:96139)\n",
      "  File \"parser.pxi\", line 582, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:91290)\n",
      "  File \"parser.pxi\", line 683, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:92476)\n",
      "  File \"parser.pxi\", line 631, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:91904)\n",
      "XMLSyntaxError: line 1140: htmlParseEntityRef: expecting ';'\n",
      "Traceback (most recent call last):\n",
      "  File \"//anaconda/lib/python2.7/site-packages/newspaper/parsers.py\", line 54, in fromstring\n",
      "    cls.doc = lxml.html.fromstring(html)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 706, in fromstring\n",
      "    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 600, in document_fromstring\n",
      "    value = etree.fromstring(html, parser, **kw)\n",
      "  File \"lxml.etree.pyx\", line 3032, in lxml.etree.fromstring (src/lxml/lxml.etree.c:68121)\n",
      "  File \"parser.pxi\", line 1786, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:102470)\n",
      "  File \"parser.pxi\", line 1667, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:101229)\n",
      "  File \"parser.pxi\", line 1035, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:96139)\n",
      "  File \"parser.pxi\", line 582, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:91290)\n",
      "  File \"parser.pxi\", line 683, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:92476)\n",
      "  File \"parser.pxi\", line 631, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:91904)\n",
      "XMLSyntaxError: line 809: htmlParseEntityRef: expecting ';'\n",
      "Traceback (most recent call last):\n",
      "  File \"//anaconda/lib/python2.7/site-packages/newspaper/parsers.py\", line 54, in fromstring\n",
      "    cls.doc = lxml.html.fromstring(html)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 706, in fromstring\n",
      "    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 600, in document_fromstring\n",
      "    value = etree.fromstring(html, parser, **kw)\n",
      "  File \"lxml.etree.pyx\", line 3032, in lxml.etree.fromstring (src/lxml/lxml.etree.c:68121)\n",
      "  File \"parser.pxi\", line 1786, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:102470)\n",
      "  File \"parser.pxi\", line 1667, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:101229)\n",
      "  File \"parser.pxi\", line 1035, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:96139)\n",
      "  File \"parser.pxi\", line 582, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:91290)\n",
      "  File \"parser.pxi\", line 683, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:92476)\n",
      "  File \"parser.pxi\", line 631, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:91904)\n",
      "XMLSyntaxError: line 986: htmlParseEntityRef: expecting ';'\n",
      "Traceback (most recent call last):\n",
      "  File \"//anaconda/lib/python2.7/site-packages/newspaper/parsers.py\", line 54, in fromstring\n",
      "    cls.doc = lxml.html.fromstring(html)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 706, in fromstring\n",
      "    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 600, in document_fromstring\n",
      "    value = etree.fromstring(html, parser, **kw)\n",
      "  File \"lxml.etree.pyx\", line 3032, in lxml.etree.fromstring (src/lxml/lxml.etree.c:68121)\n",
      "  File \"parser.pxi\", line 1786, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:102470)\n",
      "  File \"parser.pxi\", line 1667, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:101229)\n",
      "  File \"parser.pxi\", line 1035, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:96139)\n",
      "  File \"parser.pxi\", line 582, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:91290)\n",
      "  File \"parser.pxi\", line 683, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:92476)\n",
      "  File \"parser.pxi\", line 631, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:91904)\n",
      "XMLSyntaxError: line 1063: htmlParseEntityRef: expecting ';'\n",
      "Traceback (most recent call last):\n",
      "  File \"//anaconda/lib/python2.7/site-packages/newspaper/parsers.py\", line 54, in fromstring\n",
      "    cls.doc = lxml.html.fromstring(html)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 706, in fromstring\n",
      "    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 600, in document_fromstring\n",
      "    value = etree.fromstring(html, parser, **kw)\n",
      "  File \"lxml.etree.pyx\", line 3032, in lxml.etree.fromstring (src/lxml/lxml.etree.c:68121)\n",
      "  File \"parser.pxi\", line 1786, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:102470)\n",
      "  File \"parser.pxi\", line 1667, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:101229)\n",
      "  File \"parser.pxi\", line 1035, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:96139)\n",
      "  File \"parser.pxi\", line 582, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:91290)\n",
      "  File \"parser.pxi\", line 683, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:92476)\n",
      "  File \"parser.pxi\", line 631, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:91904)\n",
      "XMLSyntaxError: line 1063: htmlParseEntityRef: expecting ';'\n",
      "Traceback (most recent call last):\n",
      "  File \"//anaconda/lib/python2.7/site-packages/newspaper/parsers.py\", line 54, in fromstring\n",
      "    cls.doc = lxml.html.fromstring(html)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 706, in fromstring\n",
      "    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 600, in document_fromstring\n",
      "    value = etree.fromstring(html, parser, **kw)\n",
      "  File \"lxml.etree.pyx\", line 3032, in lxml.etree.fromstring (src/lxml/lxml.etree.c:68121)\n",
      "  File \"parser.pxi\", line 1786, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:102470)\n",
      "  File \"parser.pxi\", line 1667, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:101229)\n",
      "  File \"parser.pxi\", line 1035, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:96139)\n",
      "  File \"parser.pxi\", line 582, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:91290)\n",
      "  File \"parser.pxi\", line 683, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:92476)\n",
      "  File \"parser.pxi\", line 631, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:91904)\n",
      "XMLSyntaxError: line 809: htmlParseEntityRef: expecting ';'\n",
      "Traceback (most recent call last):\n",
      "  File \"//anaconda/lib/python2.7/site-packages/newspaper/parsers.py\", line 54, in fromstring\n",
      "    cls.doc = lxml.html.fromstring(html)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 706, in fromstring\n",
      "    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 600, in document_fromstring\n",
      "    value = etree.fromstring(html, parser, **kw)\n",
      "  File \"lxml.etree.pyx\", line 3032, in lxml.etree.fromstring (src/lxml/lxml.etree.c:68121)\n",
      "  File \"parser.pxi\", line 1786, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:102470)\n",
      "  File \"parser.pxi\", line 1667, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:101229)\n",
      "  File \"parser.pxi\", line 1035, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:96139)\n",
      "  File \"parser.pxi\", line 582, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:91290)\n",
      "  File \"parser.pxi\", line 683, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:92476)\n",
      "  File \"parser.pxi\", line 631, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:91904)\n",
      "XMLSyntaxError: line 809: htmlParseEntityRef: expecting ';'\n",
      "Traceback (most recent call last):\n",
      "  File \"//anaconda/lib/python2.7/site-packages/newspaper/parsers.py\", line 54, in fromstring\n",
      "    cls.doc = lxml.html.fromstring(html)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 706, in fromstring\n",
      "    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 600, in document_fromstring\n",
      "    value = etree.fromstring(html, parser, **kw)\n",
      "  File \"lxml.etree.pyx\", line 3032, in lxml.etree.fromstring (src/lxml/lxml.etree.c:68121)\n",
      "  File \"parser.pxi\", line 1786, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:102470)\n",
      "  File \"parser.pxi\", line 1667, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:101229)\n",
      "  File \"parser.pxi\", line 1035, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:96139)\n",
      "  File \"parser.pxi\", line 582, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:91290)\n",
      "  File \"parser.pxi\", line 683, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:92476)\n",
      "  File \"parser.pxi\", line 631, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:91904)\n",
      "XMLSyntaxError: line 1011: htmlParseEntityRef: expecting ';'\n"
     ]
    }
   ],
   "source": [
    "fox_text = []\n",
    "get_fox_text(fox_urls.url, fox_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fox_urls['text'] = fox_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('fox_text2.pkl', 'wb') as picklefile:\n",
    "#     pickle.dump(fox_text, picklefile)\n",
    "    \n",
    "# with open('fox_urls2.pkl', 'wb') as picklefile:\n",
    "#     pickle.dump(fox_urls, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('fox_urls2.pkl', 'rb') as picklefile:\n",
    "#     fox_urls = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download New York Times articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nyt_urls = get_nyt_urls()\n",
    "# The last 10 rows are from page 43. All the rest of the rows are in order of relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('nyt_urls.pkl', 'wb') as picklefile:\n",
    "#     pickle.dump(nyt_urls, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"//anaconda/lib/python2.7/site-packages/newspaper/parsers.py\", line 54, in fromstring\n",
      "    cls.doc = lxml.html.fromstring(html)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 706, in fromstring\n",
      "    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 600, in document_fromstring\n",
      "    value = etree.fromstring(html, parser, **kw)\n",
      "  File \"lxml.etree.pyx\", line 3032, in lxml.etree.fromstring (src/lxml/lxml.etree.c:68121)\n",
      "  File \"parser.pxi\", line 1786, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:102470)\n",
      "  File \"parser.pxi\", line 1667, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:101229)\n",
      "  File \"parser.pxi\", line 1035, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:96139)\n",
      "  File \"parser.pxi\", line 582, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:91290)\n",
      "  File \"parser.pxi\", line 683, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:92476)\n",
      "  File \"parser.pxi\", line 631, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:91904)\n",
      "XMLSyntaxError: line 954: Tag nav invalid\n",
      "Traceback (most recent call last):\n",
      "  File \"//anaconda/lib/python2.7/site-packages/newspaper/parsers.py\", line 54, in fromstring\n",
      "    cls.doc = lxml.html.fromstring(html)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 706, in fromstring\n",
      "    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 600, in document_fromstring\n",
      "    value = etree.fromstring(html, parser, **kw)\n",
      "  File \"lxml.etree.pyx\", line 3032, in lxml.etree.fromstring (src/lxml/lxml.etree.c:68121)\n",
      "  File \"parser.pxi\", line 1786, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:102470)\n",
      "  File \"parser.pxi\", line 1667, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:101229)\n",
      "  File \"parser.pxi\", line 1035, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:96139)\n",
      "  File \"parser.pxi\", line 582, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:91290)\n",
      "  File \"parser.pxi\", line 683, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:92476)\n",
      "  File \"parser.pxi\", line 631, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:91904)\n",
      "XMLSyntaxError: line 805: Tag nav invalid\n",
      "Traceback (most recent call last):\n",
      "  File \"//anaconda/lib/python2.7/site-packages/newspaper/parsers.py\", line 54, in fromstring\n",
      "    cls.doc = lxml.html.fromstring(html)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 706, in fromstring\n",
      "    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 600, in document_fromstring\n",
      "    value = etree.fromstring(html, parser, **kw)\n",
      "  File \"lxml.etree.pyx\", line 3032, in lxml.etree.fromstring (src/lxml/lxml.etree.c:68121)\n",
      "  File \"parser.pxi\", line 1786, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:102470)\n",
      "  File \"parser.pxi\", line 1667, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:101229)\n",
      "  File \"parser.pxi\", line 1035, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:96139)\n",
      "  File \"parser.pxi\", line 582, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:91290)\n",
      "  File \"parser.pxi\", line 683, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:92476)\n",
      "  File \"parser.pxi\", line 631, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:91904)\n",
      "XMLSyntaxError: line 471: Tag nav invalid\n",
      "Traceback (most recent call last):\n",
      "  File \"//anaconda/lib/python2.7/site-packages/newspaper/parsers.py\", line 54, in fromstring\n",
      "    cls.doc = lxml.html.fromstring(html)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 706, in fromstring\n",
      "    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)\n",
      "  File \"//anaconda/lib/python2.7/site-packages/lxml/html/__init__.py\", line 600, in document_fromstring\n",
      "    value = etree.fromstring(html, parser, **kw)\n",
      "  File \"lxml.etree.pyx\", line 3032, in lxml.etree.fromstring (src/lxml/lxml.etree.c:68121)\n",
      "  File \"parser.pxi\", line 1786, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:102470)\n",
      "  File \"parser.pxi\", line 1667, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:101229)\n",
      "  File \"parser.pxi\", line 1035, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:96139)\n",
      "  File \"parser.pxi\", line 582, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:91290)\n",
      "  File \"parser.pxi\", line 683, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:92476)\n",
      "  File \"parser.pxi\", line 631, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:91904)\n",
      "XMLSyntaxError: line 1324: Tag nav invalid\n"
     ]
    }
   ],
   "source": [
    "nyt_text = []\n",
    "get_nyt_text(nyt_urls.web_url, nyt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nyt_urls['text'] = nyt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('nyt_urls2.pkl', 'wb') as picklefile:\n",
    "#     pickle.dump(nyt_urls, picklefile)\n",
    "    \n",
    "# with open('nyt_text2.pkl', 'wb') as pickletfile:\n",
    "#     pickle.dump(nyt_text, picklefile)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
